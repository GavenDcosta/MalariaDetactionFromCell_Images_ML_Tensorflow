{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Malaria dataset from TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('malaria', \n",
    "                                         split=['train[:80%]', 'train[80%:]'],\n",
    "                                         shuffle_files=True,\n",
    "                                         as_supervised=True,  # Gets (image, label) pairs\n",
    "                                         with_info=True)\n",
    "\n",
    "# Display dataset information\n",
    "print(ds_info)\n",
    "\n",
    "# Define image parameters\n",
    "img_height, img_width = 100, 100\n",
    "batch_size = 32\n",
    "\n",
    "# Normalize the images and resize them\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    image = image / 255.0  # Normalize pixel values between 0 and 1\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "ds_train = ds_train.map(preprocess).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(preprocess).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')  # 2 classes: Parasitized and Uninfected\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(ds_train, validation_data=ds_test, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(ds_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training history\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload and predict\n",
    "def upload_and_predict(model):\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded.keys():\n",
    "        # Load the image\n",
    "        img = Image.open(filename)\n",
    "        img = img.resize((img_height, img_width))  # Resize image to match model input shape\n",
    "        img_array = np.array(img) / 255.0  # Normalize the image\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(img_array)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        \n",
    "        class_names = ['Parasitized', 'Uninfected']\n",
    "        predicted_class = class_names[np.argmax(score)]\n",
    "        confidence = 100 * np.max(score)\n",
    "        \n",
    "        print(f\"Predicted Class: {predicted_class} with {confidence:.2f}% confidence.\")\n",
    "\n",
    "# Call the function to upload and predict\n",
    "upload_and_predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to your Google Drive\n",
    "model.save('/content/drive/MyDrive/malaria_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the weights\n",
    "model.save_weights('/content/drive/MyDrive/malaria_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
